{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McF2O-KnjxnI",
        "outputId": "866a2981-a20f-4653-809d-227e7119aa11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "760.7\n"
          ]
        }
      ],
      "source": [
        "#Basic level of it is :\n",
        "inputs = [10.1,12.3, 11.1]\n",
        "weights = [30.1,20.1,18.6]\n",
        "bias=3\n",
        "\n",
        "\n",
        "output= inputs[0]*weights[0] + inputs[1]*weights[1] + inputs[2]*weights[2] + bias\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Assume real life scenario, where inputs can be from a true input value - ie- Sensors or from other neurons ig\n",
        "# Modelling a output neuron\n",
        "inputs = [1,2,3,4]\n",
        "weights = [0.2,0.8,-0.5,1.0]\n",
        "bias=3\n",
        "\n",
        "\n",
        "output= inputs[0]*weights[0] + inputs[1]*weights[1] + inputs[2]*weights[2] + inputs[3]*weights[3] + bias\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqXAK_Vkkav7",
        "outputId": "6f53c325-612f-4711-fe81-91eb285e44c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model say 3 nuerons with 4 inputs: So layer output would not be a single value!!\n",
        "inputs = [1,2,3,4]\n",
        "weights = [0.2,0.8,-0.5,1.11]\n",
        "weights1 = [0.3,0.5,-0.91,0.26]\n",
        "weights2 = [0.44,0.3,-0.4,1.4]\n",
        "\n",
        "bias=3\n",
        "bias1=6.1\n",
        "bias2=5\n",
        "output= [inputs[0]*weights[0] + inputs[1]*weights[1] + inputs[2]*weights[2] + inputs[3]*weights[3] + bias, inputs[0]*weights1[0] + inputs[1]*weights1[1] + inputs[2]*weights1[2] + inputs[3]*weights1[3] + bias, inputs[0]*weights2[0] + inputs[1]*weights2[1] + inputs[2]*weights2[2] + inputs[3]*weights2[3] + bias]\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2DMTF0MmPuM",
        "outputId": "8bbcc906-857f-4a9d-c0ce-2ce7a0357d7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7.74, 2.6100000000000003, 8.44]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Now getting our hands dirty, we have:\n",
        "#We have concept of Activation prograrms- bias is offset or (intercept jaisa for y=mx+c) So 2 different tools (bias and weight) do 2 different works.\n",
        "inputs = [1,2,3,4]\n",
        "weights = [[0.2,0.8,-0.5,1.11],[0.3,0.5,-0.91,0.26],[0.44,0.3,-0.4,1.4]]\n",
        "biases=[3, 6.1, 5]\n",
        "\n",
        "layer_outputs=[] #Output of current layer\n",
        "for nw,nb in zip(weights,biases):\n",
        "    output=0 #Output of given neuron\n",
        "    for n1,w1 in zip(inputs,nw):\n",
        "        output+=n1*w1\n",
        "    output+=nb\n",
        "    layer_outputs.append(output)\n",
        "print(layer_outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5W0DcyGx4r2",
        "outputId": "8bf33432-1d73-43e9-c9e4-846d2a3b3fd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7.74, 5.71, 10.44]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Single layer ke liye dot product\n",
        "import numpy as np\n",
        "#inputs = [1,2,3,4]\n",
        "#weights = [0.2,0.8,-0.5,1.11]\n",
        "\n",
        "#outputs= np.dot(weights, inputs)+ bias #On interchanging weights and inputs, we get shape error (I'e error in Matrix Multiplication)\n",
        "#print(outputs)\n",
        "\n",
        "inputs = [[1,2,3,4],[5,6,7,8],[9,10,11,12]]\n",
        "weights = [[0.2,0.8,-0.5,1.11],[0.3,0.5,-0.91,0.26],[0.44,0.3,-0.4,1.4]]\n",
        "# Correcting the shape of weights2 to be compatible with layer1_outputs\n",
        "weights2 = [[0.22,0.18,-0.25],[05.3,05.5,-03.91],[0.44,0.3,-0.4]] # Adjusted weights2 to have 3 columns\n",
        "biases=[3, 6.1, 5]\n",
        "biases2=[3, 6, 0.9]\n",
        "layer1_outputs= np.dot(inputs,np.array(weights).T)+ biases\n",
        "layer2_outputs= np.dot(layer1_outputs,np.array(weights2).T)+ biases2 # Now this multiplication is valid\n",
        "print(layer2_outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcYxnQEKnhMB",
        "outputId": "0ee01dec-250e-4150-be76-bc697caad52c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 3.1206 37.6066  1.8426]\n",
            " [ 2.9054 47.825   2.0722]\n",
            " [ 2.6902 58.0434  2.3018]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "X=[[1,2,3,2.5],[4,5,6,1.2],[7,8,9,1.45]]\n",
        "np.random.seed(0)\n",
        "class Layer_Dense: #We do that as Data passes through, it becomes bigger and bigger till an explosion- Scaling of the dataset.\n",
        "  def __init__(self, n_inputs, n_neurons):\n",
        "    self.weights= 0.01* np.random.randn(n_inputs ,n_neurons)#Shaping the weights\n",
        "    self.biases=np.zeros((1,n_neurons))\n",
        "  def forward(self, inputs): #Forward Method: takes the inputs\n",
        "    self.output=np.dot(inputs,self.weights) +self.biases\n",
        "\n",
        "layer_1= Layer_Dense(4,5)\n",
        "layer_2= Layer_Dense(5,2)#5 yaha same hai, zaruri for Matrices\n",
        "layer_1.forward(X)\n",
        "print(layer_1.output)\n",
        "layer_2.forward(layer_1.output)\n",
        "print(layer_2.output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65EBRPOxSFXe",
        "outputId": "313da677-c9cc-4298-d839-ea54997bd1e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.01075813 0.10398352 0.02446241 0.0318215  0.01885105]\n",
            " [0.03434491 0.16869607 0.07478202 0.0955321  0.11161489]\n",
            " [0.06310363 0.25656684 0.12192168 0.16409525 0.19114024]]\n",
            "[[ 0.00148296 -0.00083976]\n",
            " [ 0.00403334 -0.00065392]\n",
            " [ 0.006379   -0.0007635 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "X=[[1,2,3,2.5],[4,5,6,1.2],[7,8,9,1.45]]\n",
        "\n",
        "inputs =[12,24,48,60,72,13,43,5]\n",
        "output=[]\n",
        "\n",
        "for i in inputs:\n",
        "  output.append(max(0,i)) #Same work\n",
        "  #if i>0:\n",
        "    #output.append(i)\n",
        "  #elif i<=0:\n",
        "    #output.append(0)\n",
        "print(output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abBT41Vizb6I",
        "outputId": "17b4ddb7-8c4c-421b-a822-6d85e2776210"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[12, 24, 48, 60, 72, 13, 43, 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nnfs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCeaVpryAFmV",
        "outputId": "5921657d-12eb-4d5d-f4a0-b90396d84f10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nnfs\n",
            "  Downloading nnfs-0.5.1-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from nnfs) (2.0.2)\n",
            "Downloading nnfs-0.5.1-py3-none-any.whl (9.1 kB)\n",
            "Installing collected packages: nnfs\n",
            "Successfully installed nnfs-0.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import nnfs\n",
        "\n",
        "nnfs.init()\n",
        "X=[[1,2,3,2.5],[4,5,6,1.2],[7,8,9,1.45]]\n",
        "np.random.seed(0)\n",
        "class Layer_Dense:\n",
        "  def __init__(self, n_inputs, n_neurons):\n",
        "    self.weights= 0.01* np.random.randn(n_inputs ,n_neurons)\n",
        "    self.biases=np.zeros((1,n_neurons))\n",
        "  def forward(self, inputs):\n",
        "    self.output=np.dot(inputs,self.weights) +self.biases\n",
        "\n",
        "class Activation_ReLU:\n",
        "  def forward(self,inputs):\n",
        "    self.output=np.maximum(0,inputs)\n",
        "\n",
        "layer_1= Layer_Dense(4,5)\n",
        "layer_2= Layer_Dense(5,2)\n",
        "\n",
        "# Convert X to a NumPy array before passing it to the forward method\n",
        "layer_1.forward(np.array(X))\n",
        "print(layer_1.output)\n",
        "layer_2.forward(layer_1.output)\n",
        "print(layer_2.output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnxHwTc1887d",
        "outputId": "697918c1-a96d-43eb-83e6-aec76c217557"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.01075813 0.10398352 0.02446241 0.0318215  0.01885105]\n",
            " [0.03434491 0.16869606 0.07478202 0.09553209 0.11161488]\n",
            " [0.06310364 0.25656682 0.12192168 0.16409524 0.19114023]]\n",
            "[[ 0.00148296 -0.00083976]\n",
            " [ 0.00403334 -0.00065392]\n",
            " [ 0.006379   -0.0007635 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nnfs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAMk1rPjjH9Q",
        "outputId": "183f66fc-c74b-4299-937e-9f0425de72da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nnfs\n",
            "  Downloading nnfs-0.5.1-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from nnfs) (2.0.2)\n",
            "Downloading nnfs-0.5.1-py3-none-any.whl (9.1 kB)\n",
            "Installing collected packages: nnfs\n",
            "Successfully installed nnfs-0.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import nnfs\n",
        "from nnfs.datasets import spiral_data\n",
        "\n",
        "nnfs.init()\n",
        "X=[[1,2,3,2.5],[4,5,6,1.2],[7,8,9,1.45]]\n",
        "\n",
        "X,y = spiral_data(100,3)\n",
        "\n",
        "\n",
        "class Layer_Dense:\n",
        "  def __init__(self, n_inputs, n_neurons):\n",
        "    self.weights= 0.01* np.random.randn(n_inputs ,n_neurons)\n",
        "    self.biases=np.zeros((1,n_neurons))\n",
        "  def forward(self, inputs):\n",
        "    self.output=np.dot(inputs,self.weights) +self.biases\n",
        "\n",
        "class Activation_ReLU:\n",
        "  def forward(self,inputs):\n",
        "    self.output=np.maximum(0,inputs)\n",
        "\n",
        "layer_1= Layer_Dense(2,5) #No of inputs, No of neurons\n",
        "activation_1= Activation_ReLU()\n",
        "layer_1.forward(np.array(X))\n",
        "activation_1.forward(layer_1.output)\n",
        "print(activation_1.output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w23skSEvhLJF",
        "outputId": "08129550-efa5-427e-c7ba-1ee78499db94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00]\n",
            " [0.0000000e+00 0.0000000e+00 0.0000000e+00 4.6550449e-05 4.5684628e-06]\n",
            " [0.0000000e+00 5.9346880e-06 0.0000000e+00 2.0357311e-05 6.1002436e-05]\n",
            " ...\n",
            " [1.1329151e-02 0.0000000e+00 0.0000000e+00 8.1107961e-03 0.0000000e+00]\n",
            " [1.3458835e-02 0.0000000e+00 3.0949395e-03 5.6633754e-03 0.0000000e+00]\n",
            " [1.0781791e-02 0.0000000e+00 0.0000000e+00 8.7256189e-03 0.0000000e+00]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "layer_outputs=[4.8, 1.21, 2.385]\n",
        "\n",
        "E= math.e\n",
        "exp_values= []\n",
        "\n",
        "for output in layer_outputs:\n",
        "  exp_values.append(E**output)\n",
        "\n",
        "print(exp_values)\n",
        "\n",
        "norm_base= sum(exp_values)\n",
        "norm_values= []\n",
        "for value in exp_values:\n",
        "  norm_values.append(value/norm_base)\n",
        "\n",
        "print(norm_values)\n",
        "print(sum(norm_values))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8efPEC9hXvz",
        "outputId": "b243a6c5-37a1-483c-d512-58b0700a3fd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[121.51041751873483, 3.353484652549023, 10.859062664920513]\n",
            "[0.8952826639572619, 0.024708306782099374, 0.0800090292606387]\n",
            "0.9999999999999999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import nnfs\n",
        "\n",
        "nnfs.init()\n",
        "\n",
        "layer_outputs=[[4.8, 1.21, 2.385],[2.11,9.23,7.1],[3.4,5.8,7.1]]\n",
        "exp_values= np.exp(layer_outputs)\n",
        "#print(np.sum(layer_outputs, axis=1, keepdims=True)) #For shaping it\n",
        "norm_values= exp_values/np.sum(exp_values, axis=1, keepdims= True)\n",
        "print(sum(norm_values))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "is6C58zYzYR6",
        "outputId": "16cbc810-0ade-464d-9b99-1a1d58d3e349"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.91506334 1.12793109 0.95700557]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import nnfs\n",
        "\n",
        "nnfs.init()\n",
        "\n",
        "class Layer_Dense:\n",
        "  def __init__(self,n_inputs,n_neurons):\n",
        "    self.weights=0.1*np.random.randn(n_inputs,n_neurons)\n",
        "    self.biases=np.zeros((1,n_neurons))\n",
        "  def forward(self,inputs):\n",
        "    self.output=np.dot(inputs,self.weights) + self.biases\n",
        "\n",
        "class Activation_ReLU:\n",
        "  def forward(self,inputs):\n",
        "    self.output=np.maximum(0,inputs)\n",
        "\n",
        "class Activation_Softmax:\n",
        "  def forward (self,inputs):\n",
        "    exp_values =np.exp(inputs- np.max(inputs, axis=1,keepdims=True))\n",
        "    probabilities= exp_values/np.sum(exp_values, axis=1, keepdims=True)\n",
        "    self.output=probabilities\n",
        "\n",
        "X,y = spiral_data(samples=100, classes=3)\n",
        "dense1= Layer_Dense(2,3)\n",
        "activation1= Activation_ReLU()\n",
        "\n",
        "dense2=Layer_Dense(3,3)\n",
        "activation2= Activation_Softmax()\n",
        "\n",
        "dense1.forward(X)\n",
        "activation1.forward(dense1.output)\n",
        "dense2.forward(activation1.output)\n",
        "activation2.forward(dense2.output)\n",
        "\n",
        "print(activation2.output[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMmGE4M1U6nU",
        "outputId": "d45756dd-c9f1-4020-8065-15d32abc8980"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.33333334 0.33333334 0.33333334]\n",
            " [0.33331734 0.3333183  0.33336434]\n",
            " [0.3332888  0.33329153 0.33341965]\n",
            " [0.33325943 0.33326396 0.33347666]\n",
            " [0.33323312 0.33323926 0.33352762]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "softmax_output=[0.7,0.1,0.2]\n",
        "target_output= [1,0,0]\n",
        "\n",
        "loss =-(math.log(softmax_output[0])*target_output[0]+math.log(softmax_output[1])*target_output[1]+math.log(softmax_output[2])*target_output[2])\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njW1Jh4_Wb5U",
        "outputId": "04edf420-55d9-41e1-fc10-bd3090a7bd93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.35667494393873245\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import nnfs\n",
        "from nnfs.datasets import spiral_data\n",
        "\n",
        "nnfs.init()\n",
        "\n",
        "class Layer_Dense:\n",
        "  def __init__(self,n_inputs,n_neurons):\n",
        "    self.weights=0.1*np.random.randn(n_inputs,n_neurons)\n",
        "    self.biases=np.zeros((1,n_neurons))\n",
        "  def forward(self,inputs):\n",
        "    self.output=np.dot(inputs,self.weights) + self.biases\n",
        "\n",
        "class Activation_ReLU:\n",
        "  def forward(self,inputs):\n",
        "    self.output=np.maximum(0,inputs)\n",
        "\n",
        "class Activation_Softmax:\n",
        "  def forward (self,inputs):\n",
        "    exp_values =np.exp(inputs- np.max(inputs, axis=1,keepdims=True))\n",
        "    probabilities= exp_values/np.sum(exp_values, axis=1, keepdims=True)\n",
        "    self.output=probabilities\n",
        "\n",
        "class Loss:\n",
        "  def calculate(self,output,y):\n",
        "    sample_losses=self.forward(output,y)\n",
        "    data_loss=np.mean(sample_losses)\n",
        "    return data_loss\n",
        "class Loss_CategoricalCrossentropy(Loss):\n",
        "  def forward(self,y_pred,y_true):\n",
        "    samples=len(y_pred)\n",
        "    y_pred_clipped=np.clip(y_pred,1e-7,1-1e-7)#Setting limits\n",
        "    if len(y_true.shape)==1:\n",
        "      correct_confidences= y_pred_clipped[range(samples),y_true]\n",
        "    elif len(y_true.shape)==2:\n",
        "      correct_confidences=np.sum(y_pred_clipped*y_true,axis=1)\n",
        "    negative_log_likelihoods=-np.log(correct_confidences)\n",
        "    return negative_log_likelihoods\n",
        "\n",
        "\n",
        "\n",
        "X,y = spiral_data(samples=100, classes=3)\n",
        "dense1= Layer_Dense(2,3)\n",
        "activation1= Activation_ReLU()\n",
        "\n",
        "dense2=Layer_Dense(3,3)\n",
        "activation2= Activation_Softmax()\n",
        "\n",
        "dense1.forward(X)\n",
        "activation1.forward(dense1.output)\n",
        "dense2.forward(activation1.output)\n",
        "activation2.forward(dense2.output)\n",
        "\n",
        "print(activation2.output[:5])\n",
        "\n",
        "loss_function= Loss_CategoricalCrossentropy()\n",
        "loss= loss_function.calculate(activation2.output,y)\n",
        "print(\"Loss:\",loss)"
      ],
      "metadata": {
        "id": "7XiOYywAtTJm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1de93a9-6fdc-4b73-df17-cbc8427a53b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.33333334 0.33333334 0.33333334]\n",
            " [0.33331734 0.3333183  0.33336434]\n",
            " [0.3332888  0.33329153 0.33341965]\n",
            " [0.33325943 0.33326396 0.33347666]\n",
            " [0.33323312 0.33323926 0.33352762]]\n",
            "Loss: 1.098445\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import nnfs\n",
        "from nnfs.datasets import spiral_data\n",
        "\n",
        "# Initialize nnfs for consistent data generation and float32 precision\n",
        "nnfs.init()\n",
        "\n",
        "# Create dataset: 100 samples per class, 3 classes\n",
        "X, y = spiral_data(samples=100, classes=3)\n",
        "\n",
        "# Fully connected (dense) layer\n",
        "class Layer_Dense:\n",
        "    def __init__(self, n_inputs, n_neurons):\n",
        "        # Initialize weights with small random values and biases with zeros\n",
        "        self.weights = 0.1 * np.random.randn(n_inputs, n_neurons)\n",
        "        self.biases = np.zeros((1, n_neurons))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        self.output = np.dot(inputs, self.weights) + self.biases\n",
        "\n",
        "# ReLU Activation Function\n",
        "class Activation_ReLU:\n",
        "    def forward(self, inputs):\n",
        "        self.output = np.maximum(0, inputs)\n",
        "\n",
        "# Softmax Activation Function (for output layer)\n",
        "class Activation_Softmax:\n",
        "    def forward(self, inputs):\n",
        "        # Z\n",
        "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
        "        # Normalize to probabilities\n",
        "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
        "        self.output = probabilities\n",
        "\n",
        "# Categorical Cross-Entropy Loss\n",
        "class Loss_CategoricalCrossentropy:\n",
        "    def forward(self, y_pred, y_true):\n",
        "        # Number of samples\n",
        "        samples = len(y_pred)\n",
        "\n",
        "        # Clip to prevent log(0)\n",
        "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
        "\n",
        "        # Probabilities for correct classes\n",
        "        if len(y_true.shape) == 1:\n",
        "            correct_confidences = y_pred_clipped[range(samples), y_true]\n",
        "        elif len(y_true.shape) == 2:\n",
        "            correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)\n",
        "\n",
        "        # Losses\n",
        "        negative_log_likelihoods = -np.log(correct_confidences)\n",
        "        return np.mean(negative_log_likelihoods)\n",
        "\n",
        "# Build the network\n",
        "dense1 = Layer_Dense(2, 3)          # 2 inputs → 3 neurons\n",
        "activation1 = Activation_ReLU()     # ReLU after first layer\n",
        "\n",
        "dense2 = Layer_Dense(3, 3)          # 3 inputs (from dense1) → 3 output neurons\n",
        "activation2 = Activation_Softmax()  # Softmax for output\n",
        "\n",
        "\n",
        "dense1.forward(X)\n",
        "activation1.forward(dense1.output)\n",
        "\n",
        "dense2.forward(activation1.output)\n",
        "activation2.forward(dense2.output)\n",
        "\n",
        "#loss\n",
        "loss_function = Loss_CategoricalCrossentropy()\n",
        "loss = loss_function.forward(activation2.output, y)\n",
        "\n",
        "print(\"First 5 outputs (after Softmax):\\n\", activation2.output[:5])\n",
        "print(\"\\nLoss:\", loss)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzj0qDZx32Ye",
        "outputId": "8eb694ed-c7e8-4ce6-dc80-bad86a33bfe0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 outputs (after Softmax):\n",
            " [[0.33333334 0.33333334 0.33333334]\n",
            " [0.33331734 0.3333183  0.33336434]\n",
            " [0.3332888  0.33329153 0.33341965]\n",
            " [0.33325943 0.33326396 0.33347666]\n",
            " [0.33323312 0.33323926 0.33352762]]\n",
            "\n",
            "Loss: 1.098445\n"
          ]
        }
      ]
    }
  ]
}